{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# -----------------------------------------------------------------------------\n",
    "# Module name       : training of the ResNext model for audio\n",
    "# Description       : Read Functions\n",
    "# Author            : Francisco Gómez <fagomezj@unal.edu.co>, Freddy Hernández <fohernandezr@unal.edu.co>\n",
    "# Creation date     : 2025\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Note: This notebook runs locally with using NVIDIA RTX-4090 GPU (24G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate samples using already trained diffusion models\n",
    "\n",
    "Generate samples using already trained diffusion models for the different species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/user/Documents/DataScience/AnuraSoundProject/Classification')\n",
    "sys.path.append('C:/Users/user/Documents/DataScience/AnuraSoundProject/DataAugmentation/diffusion')\n",
    "sys.path.append(\"C:/Users/user/Documents/DataScience/AnuraSoundProject/DataAugmentation/diffusion/model_utils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\user\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\configuration_utils.py:314: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "########\n",
    "from audio_classification import configureModel,saveModel\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "########\n",
    "import argparse\n",
    "import json\n",
    "import models\n",
    "import training_loss\n",
    "import torch\n",
    "import dataset_manager\n",
    "from dataset_manager import GenerativeAIDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformation.data_preprocessing import get_data_composing\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "''' Load parameters like we did in the training script '''\n",
    "from collections import namedtuple\n",
    "from dataset_manager import GenerativeAIDataset\n",
    "\n",
    "\n",
    "#########\n",
    "import argparse\n",
    "import json\n",
    "import models\n",
    "import training_loss\n",
    "import torch\n",
    "import dataset_manager\n",
    "from dataset_manager import GenerativeAIDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformation.data_preprocessing import get_data_composing\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "''' Now we can generate samples using the model '''\n",
    "from sampling.sampling_utils import get_diffusion_sample\n",
    "import sys\n",
    "from model_utils.diffusion_utils import calc_diffusion_hyperparams\n",
    "from dataset_manager import GenerativeAIDataset\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "\n",
    "def saveAudiosToFolder(audios,output_dir,sample_rate,nameFile):    \n",
    "    \"\"\"\n",
    "    Saves a list of audio arrays to WAV files in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    audios : list or array-like\n",
    "        A list or array of audio signals (each as a NumPy array). Each signal should be 1D or 2D (with channel dim).\n",
    "    output_dir : str\n",
    "        The directory where the audio files will be saved. Created if it does not exist.\n",
    "    sample_rate : int\n",
    "        The sample rate (in Hz) to use when saving the WAV files.\n",
    "    nameFile : str\n",
    "        A base name for the output files. Each file will be named as \"{nameFile}_audio_{i}.wav\".\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - Converts each audio signal to float32 if not already.\n",
    "    - Uses `soundfile.write` to save each file.\n",
    "    \"\"\"\n",
    "    # Create folder if is missing\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the audio\n",
    "    for i, audio in enumerate(audios):\n",
    "        y = audio.squeeze()  \n",
    "        \n",
    "        # It is in a valid range?\n",
    "        if y.dtype != np.float32:\n",
    "            y = y.astype(np.float32)\n",
    "    \n",
    "        filename = os.path.join(output_dir, nameFile+f\"_audio_{i+1}.wav\")\n",
    "        sf.write(filename, y, sample_rate)\n",
    "    \n",
    "    print(\"Generated Audios Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration for generate samples using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Now we can generate samples using the model '''\n",
    "Args = namedtuple('Args', [\n",
    "    'batch_size',\n",
    "    'learning_rate',\n",
    "    'max_epochs',\n",
    "    'model',\n",
    "    'config'\n",
    "])\n",
    "#############################\n",
    "args = Args(\n",
    "    batch_size=8,\n",
    "    learning_rate=1e-3,\n",
    "    max_epochs=350,\n",
    "    model='DIFFUSION',\n",
    "    config='./config.json')\n",
    "\n",
    "\n",
    "pathDiffusionAugmentedData = \"C:/Users/user/Documents/soundDBAugmented/generated_by_diffusion\"\n",
    "nsamplesToGenerate = 200\n",
    "sr = 16000\n",
    "labelsClasses = ['s1','s2','s3','s4','s5','s6','s7','s8','s9']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
